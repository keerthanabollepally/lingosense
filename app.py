#############final(telugu , hindi, marathi, bengali ,tamil)
# =========================
# Imports
# =========================
import re
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from indicnlp.normalize import indic_normalize
from indic_transliteration import sanscript
from indic_transliteration.sanscript import transliterate

# =========================
# Device Setup
# =========================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# =========================
# Model Loading
# =========================
MODEL_NLLB = "facebook/nllb-200-distilled-600M"
tokenizer_nllb = AutoTokenizer.from_pretrained(MODEL_NLLB)
model_nllb = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NLLB).to(DEVICE)

# (Optional: Load IndicTrans if needed for Hindi/Tamil/Telugu and English bridging)

# =========================
# Settings (language parameters)
# =========================
LANG_CONFIG = {
    "hindi": {
        "code_mix_dict": {
            "class": "‡§ï‡§ï‡•ç‡§∑‡§æ", "meeting": "‡§¨‡•à‡§†‡§ï", "me": "‡§Æ‡•á‡§Ç", "after": "‡§¨‡§æ‡§¶", "ke": "‡§ï‡•á", "baad": "‡§¨‡§æ‡§¶",
            "aana": "‡§Ü‡§®‡§æ", "hai": "‡§π‡•à", "mujhe": "‡§Æ‡•Å‡§ù‡•á", "please": "‡§ï‡•É‡§™‡§Ø‡§æ", "sir": "‡§∏‡§æ‡§π‡§¨", "madam": "‡§Æ‡•à‡§°‡§Æ"
        },
        "src_script": sanscript.ITRANS,
        "tgt_script": sanscript.DEVANAGARI,
        "lang_code_nllb": "hin_Deva",
        "normalizer": "hi"
    },
    "tamil": {
        "code_mix_dict": {
            "naan": "‡Æ®‡Ææ‡Æ©‡Øç", "nee": "‡Æ®‡ØÄ", "class": "‡Æï‡Æø‡Æ≥‡Ææ‡Æ∏‡Øç", "ku": "‡Æï‡Øç‡Æï‡ØÅ", "poganum": "‡Æ™‡Øã‡Æï‡Æ£‡ØÅ‡ÆÆ‡Øç",
            "ponanum": "‡Æ™‡Øã‡Æµ‡Øá‡Æ©‡Øç", "varanum": "‡Æµ‡Æ∞‡Æ£‡ØÅ‡ÆÆ‡Øç", "sollanum": "‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡Æ£‡ØÅ‡ÆÆ‡Øç", "pananum": "‡Æ™‡Æ£‡Æ£‡ØÅ‡ÆÆ‡Øç"
        },
        "src_script": sanscript.ITRANS,
        "tgt_script": sanscript.TAMIL,
        "lang_code_nllb": "tam_Taml",
        "normalizer": "ta"
    },
    "telugu": {
        "code_mix_dict": {
            "class": "‡∞§‡∞∞‡∞ó‡∞§‡∞ø", "vellali": "‡∞µ‡±Ü‡∞≥‡±ç‡∞≤‡∞æ‡∞≤‡∞ø"
        },
        "src_script": sanscript.ITRANS,
        "tgt_script": sanscript.TELUGU,
        "lang_code_nllb": "tel_Telu",
        "normalizer": "te"
    },
    "marathi": {
        "code_mix_dict": {
            "tu": "‡§§‡•Ç", "tula": "‡§§‡•Å‡§≤‡§æ", "kasa": "‡§ï‡§∏‡§æ", "kasaa": "‡§ï‡§∏‡§æ", "aahe": "‡§Ü‡§π‡•á", "aahes": "‡§Ü‡§π‡•á‡§∏", "majha": "‡§Æ‡§æ‡§ù‡§æ",
            "mazi": "‡§Æ‡§æ‡§ù‡•Ä", "maza": "‡§Æ‡§æ‡§ù‡§æ", "bandhu": "‡§≠‡§æ‡§ä", "mitra": "‡§Æ‡§ø‡§§‡•ç‡§∞", "aaj": "‡§Ü‡§ú", "udya": "‡§â‡§¶‡•ç‡§Ø‡§æ",
            "school": "‡§∂‡§æ‡§≥‡§æ", "la": "‡§≤‡§æ", "nako": "‡§®‡§ï‡•ã", "karan": "‡§ï‡§æ‡§∞‡§£", "majha dost": "‡§Æ‡§æ‡§ù‡§æ ‡§Æ‡§ø‡§§‡•ç‡§∞",
            "maza dost": "‡§Æ‡§æ‡§ù‡§æ ‡§Æ‡§ø‡§§‡•ç‡§∞", "bhet": "‡§≠‡•á‡§ü", "chhan": "‡§õ‡§æ‡§®", "movie": "‡§ö‡§ø‡§§‡•ç‡§∞‡§™‡§ü",
            "pahila": "‡§™‡§æ‡§π‡§ø‡§≤‡§æ", "pahile": "‡§™‡§æ‡§π‡§ø‡§≤‡•á", "awesome": "‡§õ‡§æ‡§®", "mi": "‡§Æ‡•Ä", "jaato": "‡§ú‡§æ‡§§‡•ã",
            "jatoy": "‡§ú‡§æ‡§§‡•ã", "nahi": "‡§®‡§æ‡§π‡•Ä", "aahe ka": "‡§Ü‡§π‡•á ‡§ï‡§æ", "dokyacha": "‡§°‡•ã‡§ï‡•ç‡§Ø‡§æ‡§ö‡§æ",
            "dukh": "‡§¶‡•Å‡§ñ", "hota": "‡§π‡•ã‡§§‡§æ", "mala": "‡§Æ‡§≤‡§æ"
        },
        "src_script": sanscript.ITRANS,
        "tgt_script": sanscript.DEVANAGARI,
        "lang_code_nllb": "mar_Deva",
        "normalizer": "mr"
    },
    "bengali": {
        "code_mix_dict": {
            "ami": "‡¶Ü‡¶Æ‡¶ø", "tumi": "‡¶§‡ßÅ‡¶Æ‡¶ø", "tomar": "‡¶§‡ßã‡¶Æ‡¶æ‡¶∞", "ke": "‡¶ï‡ßá",
            "sathe": "‡¶∏‡¶æ‡¶•‡ßá", "bhalo": "‡¶≠‡¶æ‡¶≤‡ßã", "jabo": "‡¶Ø‡¶æ‡¶¨‡ßã", "asche": "‡¶Ü‡¶∏‡ßá",
            "korbo": "‡¶ï‡¶∞‡¶¨‡ßã", "amar": "‡¶Ü‡¶Æ‡¶æ‡¶∞", "kotha": "‡¶ï‡ßã‡¶•‡¶æ", "bari": "‡¶¨‡¶æ‡¶°‡¶º‡¶ø",
            "achho": "‡¶Ü‡¶õ‡ßã", "ki": "‡¶ï‡¶ø"
        },
        "src_script": sanscript.ITRANS,
        "tgt_script": sanscript.BENGALI,
        "lang_code_nllb": "ben_Beng",
        "normalizer": "bn"
    }
    
}

# =========================
# Core Functions (per language)
# =========================

def transliterate_roman_to_native(text: str, lang: str) -> str:
    config = LANG_CONFIG[lang]
    tokens = re.findall(r"\w+|[^\w\s]", text, flags=re.UNICODE)
    out_tokens = []
    for tok in tokens:
        if re.match(r"^[A-Za-z]+$", tok):
            low = tok.lower()
            mapped = config["code_mix_dict"].get(low)
            if mapped:
                out_tokens.append(mapped)
            else:
                try:
                    native = transliterate(tok, config["src_script"], config["tgt_script"])
                    if re.search(r"[A-Za-z]", native):
                        out_tokens.append(tok)
                    else:
                        out_tokens.append(native)
                except Exception:
                    out_tokens.append(tok)
        else:
            out_tokens.append(tok)
    return " ".join(out_tokens).strip()

def normalize_native_text(text: str, lang: str) -> str:
    normalizer = indic_normalize.IndicNormalizerFactory().get_normalizer(LANG_CONFIG[lang]["normalizer"])
    normalized = normalizer.normalize(text)
    # Add custom replacements per language, e.g., ("‡§ï‡•ç‡§≤‡§æ‡§∏", "‡§ï‡§ï‡•ç‡§∑‡§æ") for Hindi
    if lang == "hindi":
        normalized = normalized.replace("‡§ï‡•ç‡§≤‡§æ‡§∏", "‡§ï‡§ï‡•ç‡§∑‡§æ").replace("‡§Æ‡•Ä‡§ü‡§ø‡§Ç‡§ó", "‡§¨‡•à‡§†‡§ï")
    elif lang == "bengali":
        normalized = normalized.replace("‡¶≠‡¶æ‡¶≤", "‡¶≠‡¶æ‡¶≤‡ßã")
    elif lang == "marathi":
        normalized = normalized.replace("‡§õ‡•ç‡§π‡§æ‡§®", "‡§õ‡§æ‡§®")
        normalized = normalized.replace("\u200c", "").replace("\u200b", "")
    return re.sub(r"\s+", " ", normalized).strip()

def detect_code_mixed_words(text: str, lang: str):
    config = LANG_CONFIG[lang]
    tokens = re.findall(r"\w+|[^\w\s]", text, flags=re.UNICODE)
    return [tok for tok in tokens if tok.lower() in config["code_mix_dict"] or re.match(r"^[A-Za-z]+$", tok)]

# =========================
# Translation (NLLB)
# =========================

def translate_nllb(text, src_lang, tgt_lang):
    tokenizer_nllb.src_lang = src_lang
    encoded = tokenizer_nllb(text, return_tensors="pt").to(DEVICE)
    tgt_lang_id = tokenizer_nllb.convert_tokens_to_ids(tgt_lang)
    generated = model_nllb.generate(**encoded, forced_bos_token_id=tgt_lang_id)
    return tokenizer_nllb.decode(generated[0], skip_special_tokens=True)

# =========================
# Combined Multilingual Pipeline
# =========================

def full_pipeline(input_text: str, input_lang: str, target_langs: list):
    print(f"\nü™Ñ Step 0: Input Text: {input_text}")

    # Step 1: Roman ‚Üí Native Script
    native_text = transliterate_roman_to_native(input_text, input_lang)
    print(f"üà∂ Step 1: After Transliteration: {native_text}")

    # Step 2: Detect code-mixed words
    code_mix = detect_code_mixed_words(input_text, input_lang)
    print(f"üîç Step 2: Detected code-mixed words: {code_mix}")

    # Step 3: Normalization
    normalized_native = normalize_native_text(native_text, input_lang)
    print(f"ü™∂ Step 3: Normalized Text: {normalized_native}")

    # Step 4: Native ‚Üí English
    english_text = translate_nllb(normalized_native, LANG_CONFIG[input_lang]["lang_code_nllb"], "eng_Latn")
    print(f"üá¨üáß Step 4: English Translation: {english_text}")

    # Step 5: English ‚Üí Target Languages
    translations = {LANG_CONFIG[input_lang]["lang_code_nllb"]: normalized_native, "eng_Latn": english_text}
    print("\nüåç Step 5: Translations to Other Languages:")
    for lang in target_langs:
        lang_code = LANG_CONFIG[lang]["lang_code_nllb"]
        translated_text = translate_nllb(english_text, "eng_Latn", lang_code)
        translations[lang_code] = translated_text
        print(f"{lang_code}: {translated_text}")

    print("\n‚úÖ Multilingual translation pipeline complete!")
    return translations

# =========================
# Example Usage (single entry point)
# =========================
if __name__ == "__main__":
    # Set source and target languages as desired
    INPUT_SENTENCE = "mujhe class ke baad meeting me aana hai"
    INPUT_LANG = "hindi"
    TARGET_LANGUAGES = ["tamil", "telugu", "marathi", "bengali"]
    results = full_pipeline(INPUT_SENTENCE, INPUT_LANG, TARGET_LANGUAGES)
